<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="CopilotDiffPersistence">
    <option name="pendingDiffs">
      <map>
        <entry key="$PROJECT_DIR$/train.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/train.py" />
              <option name="originalContent" value="&quot;&quot;&quot;&#10;Rock Paper Scissors - Model Training Module&#10;Train a classifier to recognize rock, paper, scissors gestures&#10;&quot;&quot;&quot;&#10;&#10;# =====================================&#10;# Import thư viện&#10;# =====================================&#10;import os&#10;import cv2&#10;import numpy as np&#10;from sklearn.linear_model import RidgeClassifier&#10;from sklearn.metrics import accuracy_score, classification_report, confusion_matrix&#10;from sklearn.preprocessing import StandardScaler&#10;from sklearn.model_selection import GridSearchCV, cross_val_score&#10;from joblib import dump&#10;import matplotlib.pyplot as plt&#10;import seaborn as sns&#10;&#10;# Import module chung&#10;from hand_feature_extractor import HandFeatureExtractor, ImageAugmentor&#10;&#10;&#10;# =====================================&#10;# Dataset Loader Class&#10;# =====================================&#10;class RPSDatasetLoader:&#10;    &quot;&quot;&quot;Class for loading and augmenting Rock Paper Scissors dataset&quot;&quot;&quot;&#10;&#10;    def __init__(self, feature_extractor, image_augmentor=None):&#10;        &quot;&quot;&quot;&#10;        Initialize dataset loader&#10;&#10;        Args:&#10;            feature_extractor: HandFeatureExtractor instance&#10;            image_augmentor: ImageAugmentor instance for data augmentation&#10;        &quot;&quot;&quot;&#10;        self.feature_extractor = feature_extractor&#10;        self.image_augmentor = image_augmentor&#10;        self.labels = {&quot;rock&quot;: 0, &quot;paper&quot;: 1, &quot;scissors&quot;: 2}&#10;&#10;    def load_dataset(self, folder_path, label, augment=False):&#10;        &quot;&quot;&quot;&#10;        Load dataset from folder&#10;&#10;        Args:&#10;            folder_path: Path to folder containing images&#10;            label: Label index for this class&#10;            augment: Whether to apply data augmentation&#10;&#10;        Returns:&#10;            Tuple of (features_list, labels_list)&#10;        &quot;&quot;&quot;&#10;        X, y = [], []&#10;        count = 0&#10;        failed = 0&#10;&#10;        for filename in os.listdir(folder_path):&#10;            if filename.endswith(('.jpg', '.png', '.jpeg')):&#10;                path = os.path.join(folder_path, filename)&#10;&#10;                if augment and self.image_augmentor:&#10;                    # Load image for augmentation&#10;                    image = cv2.imread(path)&#10;                    if image is None:&#10;                        failed += 1&#10;                        continue&#10;&#10;                    # Apply image augmentation (rotate, flip)&#10;                    augmented_images = self.image_augmentor.augment_image(image)&#10;&#10;                    for aug_img in augmented_images:&#10;                        features = self.feature_extractor.extract_features_from_image(aug_img)&#10;                        if features is not None:&#10;                            # Apply feature-level noise augmentation&#10;                            feature_augmented = self.image_augmentor.augment_features(features, num_augmentations=1)&#10;                            for aug_feat in feature_augmented:&#10;                                X.append(aug_feat)&#10;                                y.append(label)&#10;                            count += len(feature_augmented)&#10;                        else:&#10;                            failed += 1&#10;                else:&#10;                    # No augmentation&#10;                    features = self.feature_extractor.extract_features_from_file(path)&#10;                    if features is not None:&#10;                        X.append(features)&#10;                        y.append(label)&#10;                        count += 1&#10;                    else:&#10;                        failed += 1&#10;&#10;        print(f&quot;   Loaded: {count} samples, Failed: {failed}&quot;)&#10;        return X, y&#10;&#10;    def load_train_test_split(self, train_root, test_root, augment_train=True):&#10;        &quot;&quot;&quot;&#10;        Load both training and test datasets&#10;&#10;        Args:&#10;            train_root: Root folder for training data&#10;            test_root: Root folder for test data&#10;            augment_train: Whether to augment training data&#10;&#10;        Returns:&#10;            Tuple of (X_train, y_train, X_test, y_test)&#10;        &quot;&quot;&quot;&#10;        X_train, y_train, X_test, y_test = [], [], [], []&#10;&#10;        print(&quot;\n Loading Training Data&quot; + (&quot; (augmented)&quot; if augment_train else &quot;&quot;))&#10;        for label_name, label_idx in self.labels.items():&#10;            print(f&quot;  {label_name}:&quot;, end=&quot; &quot;)&#10;            X1, y1 = self.load_dataset(&#10;                os.path.join(train_root, label_name),&#10;                label_idx,&#10;                augment=augment_train&#10;            )&#10;            X_train.extend(X1)&#10;            y_train.extend(y1)&#10;&#10;        print(&quot;\n Loading Test Data&quot;)&#10;        for label_name, label_idx in self.labels.items():&#10;            print(f&quot;  {label_name}:&quot;, end=&quot; &quot;)&#10;            X2, y2 = self.load_dataset(&#10;                os.path.join(test_root, label_name),&#10;                label_idx,&#10;                augment=False&#10;            )&#10;            X_test.extend(X2)&#10;            y_test.extend(y2)&#10;&#10;        return (np.array(X_train), np.array(y_train),&#10;                np.array(X_test), np.array(y_test))&#10;&#10;&#10;# =====================================&#10;# Model Trainer Class&#10;# =====================================&#10;class RPSModelTrainer:&#10;    &quot;&quot;&quot;Class for training and evaluating Rock Paper Scissors classifier&quot;&quot;&quot;&#10;&#10;    def __init__(self, model=None, scaler=None):&#10;        &quot;&quot;&quot;&#10;        Initialize trainer&#10;&#10;        Args:&#10;            model: Sklearn classifier (default: RidgeClassifier)&#10;            scaler: Feature scaler (default: StandardScaler)&#10;        &quot;&quot;&quot;&#10;        self.model = model if model else RidgeClassifier(random_state=42)&#10;        self.scaler = scaler if scaler else StandardScaler()&#10;        self.labels = {0: &quot;rock&quot;, 1: &quot;paper&quot;, 2: &quot;scissors&quot;}&#10;        self.best_model = None&#10;&#10;    def normalize_data(self, X_train, X_test):&#10;        &quot;&quot;&quot;&#10;        Normalize features using StandardScaler&#10;&#10;        Args:&#10;            X_train: Training features&#10;            X_test: Test features&#10;&#10;        Returns:&#10;            Tuple of (X_train_scaled, X_test_scaled)&#10;        &quot;&quot;&quot;&#10;        print(&quot;\n⚙️ Normalizing features...&quot;)&#10;        X_train_scaled = self.scaler.fit_transform(X_train)&#10;        X_test_scaled = self.scaler.transform(X_test)&#10;        return X_train_scaled, X_test_scaled&#10;&#10;    def hyperparameter_tuning(self, X_train, y_train, param_grid=None):&#10;        &quot;&quot;&quot;&#10;        Perform hyperparameter tuning using GridSearchCV&#10;&#10;        Args:&#10;            X_train: Training features (normalized)&#10;            y_train: Training labels&#10;            param_grid: Grid of parameters to search (optional)&#10;&#10;        Returns:&#10;            Best estimator found&#10;        &quot;&quot;&quot;&#10;        if param_grid is None:&#10;            param_grid = {&#10;                'alpha': [0.1, 1.0, 10.0, 100.0],&#10;                'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']&#10;            }&#10;&#10;        print(&quot;\n Performing hyperparameter tuning (this may take a while)...&quot;)&#10;&#10;        grid_search = GridSearchCV(&#10;            self.model,&#10;            param_grid,&#10;            cv=5,&#10;            scoring='accuracy',&#10;            n_jobs=-1,&#10;            verbose=1&#10;        )&#10;&#10;        grid_search.fit(X_train, y_train)&#10;&#10;        print(f&quot;\n✨ Best parameters found: {grid_search.best_params_}&quot;)&#10;        print(f&quot;✨ Best cross-validation score: {grid_search.best_score_:.4f}&quot;)&#10;&#10;        self.best_model = grid_search.best_estimator_&#10;        return self.best_model&#10;&#10;    def cross_validate(self, X_train, y_train, cv=5):&#10;        &quot;&quot;&quot;&#10;        Perform cross-validation&#10;&#10;        Args:&#10;            X_train: Training features (normalized)&#10;            y_train: Training labels&#10;            cv: Number of folds&#10;&#10;        Returns:&#10;            Cross-validation scores&#10;        &quot;&quot;&quot;&#10;        print(&quot;\n Cross-validation scores:&quot;)&#10;        model = self.best_model if self.best_model else self.model&#10;        cv_scores = cross_val_score(model, X_train, y_train, cv=cv)&#10;        print(f&quot;   CV Scores: {cv_scores}&quot;)&#10;        print(f&quot;   Mean CV Score: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})&quot;)&#10;        return cv_scores&#10;&#10;    def evaluate(self, X_test, y_test):&#10;        &quot;&quot;&quot;&#10;        Evaluate model on test set&#10;&#10;        Args:&#10;            X_test: Test features (normalized)&#10;            y_test: Test labels&#10;&#10;        Returns:&#10;            Tuple of (accuracy, predictions)&#10;        &quot;&quot;&quot;&#10;        print(&quot;\n Evaluating on test set...&quot;)&#10;        model = self.best_model if self.best_model else self.model&#10;        y_pred = model.predict(X_test)&#10;        acc = accuracy_score(y_test, y_pred)&#10;&#10;        print(f&quot;\n Test accuracy: {acc:.4f}\n&quot;)&#10;        print(&quot;Classification report:&quot;)&#10;        print(classification_report(y_test, y_pred, target_names=self.labels.values()))&#10;&#10;        return acc, y_pred&#10;&#10;    def plot_confusion_matrix(self, y_test, y_pred, save_path='model/confusion_matrix.png'):&#10;        &quot;&quot;&quot;&#10;        Plot and save confusion matrix&#10;&#10;        Args:&#10;            y_test: True labels&#10;            y_pred: Predicted labels&#10;            save_path: Path to save the plot&#10;        &quot;&quot;&quot;&#10;        cm = confusion_matrix(y_test, y_pred)&#10;        plt.figure(figsize=(8, 6))&#10;        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',&#10;                    xticklabels=self.labels.values(),&#10;                    yticklabels=self.labels.values())&#10;        plt.title('Confusion Matrix')&#10;        plt.ylabel('True Label')&#10;        plt.xlabel('Predicted Label')&#10;&#10;        os.makedirs(os.path.dirname(save_path), exist_ok=True)&#10;        plt.savefig(save_path, dpi=150, bbox_inches='tight')&#10;        print(f&quot; Confusion matrix saved to {save_path}&quot;)&#10;        plt.close()&#10;&#10;    def save_model(self, model_path='model/rps_ridge_model.joblib',&#10;                   scaler_path='model/rps_scaler.joblib'):&#10;        &quot;&quot;&quot;&#10;        Save trained model and scaler&#10;&#10;        Args:&#10;            model_path: Path to save model&#10;            scaler_path: Path to save scaler&#10;        &quot;&quot;&quot;&#10;        model = self.best_model if self.best_model else self.model&#10;&#10;        os.makedirs(os.path.dirname(model_path), exist_ok=True)&#10;        dump(model, model_path)&#10;        dump(self.scaler, scaler_path)&#10;&#10;        print(f&quot;\n Saved model to {model_path}&quot;)&#10;        print(f&quot; Saved scaler to {scaler_path}&quot;)&#10;&#10;&#10;# =====================================&#10;# Main Training Pipeline&#10;# =====================================&#10;class TrainingPipeline:&#10;    &quot;&quot;&quot;Complete training pipeline for RPS classifier&quot;&quot;&quot;&#10;&#10;    def __init__(self, train_root, test_root,&#10;                 static_image_mode=True, max_num_hands=1,&#10;                 min_detection_confidence=0.3, min_tracking_confidence=0.3):&#10;        &quot;&quot;&quot;&#10;        Initialize training pipeline&#10;&#10;        Args:&#10;            train_root: Root folder for training data&#10;            test_root: Root folder for test data&#10;            static_image_mode: Whether to treat input as static images&#10;            max_num_hands: Maximum number of hands to detect&#10;            min_detection_confidence: Minimum confidence for hand detection&#10;            min_tracking_confidence: Minimum confidence for hand tracking&#10;        &quot;&quot;&quot;&#10;        self.train_root = train_root&#10;        self.test_root = test_root&#10;&#10;        # Initialize components&#10;        self.feature_extractor = HandFeatureExtractor(&#10;            static_image_mode=static_image_mode,&#10;            max_num_hands=max_num_hands,&#10;            min_detection_confidence=min_detection_confidence,&#10;            min_tracking_confidence=min_tracking_confidence&#10;        )&#10;        self.image_augmentor = ImageAugmentor()&#10;        self.dataset_loader = RPSDatasetLoader(self.feature_extractor, self.image_augmentor)&#10;        self.trainer = RPSModelTrainer()&#10;&#10;    def run(self, augment_train=True, tune_hyperparameters=True,&#10;            param_grid=None, model_path='model/rps_ridge_model.joblib',&#10;            scaler_path='model/rps_scaler.joblib',&#10;            confusion_matrix_path='model/confusion_matrix.png'):&#10;        &quot;&quot;&quot;&#10;        Run complete training pipeline&#10;&#10;        Args:&#10;            augment_train: Whether to augment training data&#10;            tune_hyperparameters: Whether to perform hyperparameter tuning&#10;            param_grid: Grid of parameters for hyperparameter tuning&#10;            model_path: Path to save trained model&#10;            scaler_path: Path to save scaler&#10;            confusion_matrix_path: Path to save confusion matrix plot&#10;        &quot;&quot;&quot;&#10;        print(&quot;=&quot; * 60)&#10;        print(&quot; Rock Paper Scissors - Model Training Pipeline&quot;)&#10;        print(&quot;=&quot; * 60)&#10;&#10;        # Load data&#10;        X_train, y_train, X_test, y_test = self.dataset_loader.load_train_test_split(&#10;            self.train_root, self.test_root, augment_train=augment_train&#10;        )&#10;&#10;        print(f&quot;\n✅ Total Train samples: {len(X_train)} | Test samples: {len(X_test)}&quot;)&#10;        print(f&quot;   Feature dimension: {X_train.shape[1]}&quot;)&#10;&#10;        # Normalize data&#10;        X_train_scaled, X_test_scaled = self.trainer.normalize_data(X_train, X_test)&#10;&#10;        # Train model&#10;        if tune_hyperparameters:&#10;            self.trainer.hyperparameter_tuning(X_train_scaled, y_train, param_grid)&#10;        else:&#10;            print(&quot;\n Training model with default parameters...&quot;)&#10;            self.trainer.model.fit(X_train_scaled, y_train)&#10;&#10;        # Cross-validation&#10;        self.trainer.cross_validate(X_train_scaled, y_train)&#10;&#10;        # Evaluate&#10;        acc, y_pred = self.trainer.evaluate(X_test_scaled, y_test)&#10;&#10;        # Plot confusion matrix&#10;        self.trainer.plot_confusion_matrix(y_test, y_pred, confusion_matrix_path)&#10;&#10;        # Save model&#10;        self.trainer.save_model(model_path, scaler_path)&#10;&#10;        # Cleanup&#10;        self.feature_extractor.close()&#10;&#10;        print(&quot;\n Training complete!&quot;)&#10;        print(&quot;=&quot; * 60)&#10;&#10;        return acc&#10;&#10;&#10;# =====================================&#10;# Main entry point&#10;# =====================================&#10;def main(train_root, test_root, augment_train=True, tune_hyperparameters=True,&#10;         param_grid=None, model_path='model/rps_ridge_model.joblib',&#10;         scaler_path='model/rps_scaler.joblib',&#10;         confusion_matrix_path='model/confusion_matrix.png',&#10;         static_image_mode=True, max_num_hands=1,&#10;         min_detection_confidence=0.3, min_tracking_confidence=0.3):&#10;    &quot;&quot;&quot;&#10;    Main function to run training pipeline&#10;&#10;    Args:&#10;        train_root: Root folder for training data&#10;        test_root: Root folder for test data&#10;        augment_train: Whether to augment training data&#10;        tune_hyperparameters: Whether to perform hyperparameter tuning&#10;        param_grid: Grid of parameters for hyperparameter tuning&#10;        model_path: Path to save trained model&#10;        scaler_path: Path to save scaler&#10;        confusion_matrix_path: Path to save confusion matrix plot&#10;        static_image_mode: Whether to treat input as static images&#10;        max_num_hands: Maximum number of hands to detect&#10;        min_detection_confidence: Minimum confidence for hand detection&#10;        min_tracking_confidence: Minimum confidence for hand tracking&#10;    &quot;&quot;&quot;&#10;    pipeline = TrainingPipeline(&#10;        train_root, test_root,&#10;        static_image_mode, max_num_hands,&#10;        min_detection_confidence, min_tracking_confidence&#10;    )&#10;    pipeline.run(&#10;        augment_train=augment_train,&#10;        tune_hyperparameters=tune_hyperparameters,&#10;        param_grid=param_grid,&#10;        model_path=model_path,&#10;        scaler_path=scaler_path,&#10;        confusion_matrix_path=confusion_matrix_path&#10;    )&#10;&#10;&#10;if __name__ == &quot;__main__&quot;:&#10;    # =====================================&#10;    # Configuration parameters&#10;    # =====================================&#10;&#10;    # Dataset paths&#10;    TRAIN_ROOT = &quot;dataset/rps&quot;&#10;    TEST_ROOT = &quot;dataset/rps-test-set&quot;&#10;&#10;    # Model save paths&#10;    MODEL_PATH = &quot;model/rps_ridge_model.joblib&quot;&#10;    SCALER_PATH = &quot;model/rps_scaler.joblib&quot;&#10;    CONFUSION_MATRIX_PATH = &quot;model/confusion_matrix.png&quot;&#10;&#10;    # Training options&#10;    AUGMENT_TRAIN = True&#10;    TUNE_HYPERPARAMETERS = True&#10;&#10;    # Hyperparameter tuning grid&#10;    PARAM_GRID = {&#10;        'alpha': [0.1, 1.0, 10.0, 100.0],&#10;        'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']&#10;    }&#10;&#10;    # MediaPipe configuration&#10;    STATIC_IMAGE_MODE = True&#10;    MAX_NUM_HANDS = 1&#10;    MIN_DETECTION_CONFIDENCE = 0.3&#10;    MIN_TRACKING_CONFIDENCE = 0.3&#10;&#10;    # Run training&#10;    main(&#10;        train_root=TRAIN_ROOT,&#10;        test_root=TEST_ROOT,&#10;        augment_train=AUGMENT_TRAIN,&#10;        tune_hyperparameters=TUNE_HYPERPARAMETERS,&#10;        param_grid=PARAM_GRID,&#10;        model_path=MODEL_PATH,&#10;        scaler_path=SCALER_PATH,&#10;        confusion_matrix_path=CONFUSION_MATRIX_PATH,&#10;        static_image_mode=STATIC_IMAGE_MODE,&#10;        max_num_hands=MAX_NUM_HANDS,&#10;        min_detection_confidence=MIN_DETECTION_CONFIDENCE,&#10;        min_tracking_confidence=MIN_TRACKING_CONFIDENCE&#10;    )&#10;" />
              <option name="updatedContent" value="&quot;&quot;&quot;&#10;Rock Paper Scissors - Model Training Module&#10;Train a classifier to recognize rock, paper, scissors gestures&#10;&quot;&quot;&quot;&#10;&#10;# =====================================&#10;# Import thư viện&#10;# =====================================&#10;import os&#10;import cv2&#10;import numpy as np&#10;from sklearn.linear_model import RidgeClassifier&#10;from sklearn.metrics import accuracy_score, classification_report, confusion_matrix&#10;from sklearn.preprocessing import StandardScaler&#10;from sklearn.model_selection import GridSearchCV, cross_val_score&#10;from joblib import dump&#10;import matplotlib.pyplot as plt&#10;import seaborn as sns&#10;&#10;# Import module chung&#10;from hand_feature_extractor import HandFeatureExtractor, ImageAugmentor&#10;&#10;&#10;# =====================================&#10;# Dataset Loader Class&#10;# =====================================&#10;class RPSDatasetLoader:&#10;    &quot;&quot;&quot;Class for loading and augmenting Rock Paper Scissors dataset&quot;&quot;&quot;&#10;&#10;    def __init__(self, feature_extractor, image_augmentor=None):&#10;        &quot;&quot;&quot;&#10;        Initialize dataset loader&#10;&#10;        Args:&#10;            feature_extractor: HandFeatureExtractor instance&#10;            image_augmentor: ImageAugmentor instance for data augmentation&#10;        &quot;&quot;&quot;&#10;        self.feature_extractor = feature_extractor&#10;        self.image_augmentor = image_augmentor&#10;        self.labels = {&quot;rock&quot;: 0, &quot;paper&quot;: 1, &quot;scissors&quot;: 2}&#10;&#10;    def load_dataset(self, folder_path, label, augment=False):&#10;        &quot;&quot;&quot;&#10;        Load dataset from folder&#10;&#10;        Args:&#10;            folder_path: Path to folder containing images&#10;            label: Label index for this class&#10;            augment: Whether to apply data augmentation&#10;&#10;        Returns:&#10;            Tuple of (features_list, labels_list)&#10;        &quot;&quot;&quot;&#10;        X, y = [], []&#10;        count = 0&#10;        failed = 0&#10;&#10;        for filename in os.listdir(folder_path):&#10;            if filename.endswith(('.jpg', '.png', '.jpeg')):&#10;                path = os.path.join(folder_path, filename)&#10;&#10;                if augment and self.image_augmentor:&#10;                    # Load image for augmentation&#10;                    image = cv2.imread(path)&#10;                    if image is None:&#10;                        failed += 1&#10;                        continue&#10;&#10;                    # Apply image augmentation (rotate, flip)&#10;                    augmented_images = self.image_augmentor.augment_image(image)&#10;&#10;                    for aug_img in augmented_images:&#10;                        features = self.feature_extractor.extract_features_from_image(aug_img)&#10;                        if features is not None:&#10;                            # Apply feature-level noise augmentation&#10;                            feature_augmented = self.image_augmentor.augment_features(features, num_augmentations=1)&#10;                            for aug_feat in feature_augmented:&#10;                                X.append(aug_feat)&#10;                                y.append(label)&#10;                            count += len(feature_augmented)&#10;                        else:&#10;                            failed += 1&#10;                else:&#10;                    # No augmentation&#10;                    features = self.feature_extractor.extract_features_from_file(path)&#10;                    if features is not None:&#10;                        X.append(features)&#10;                        y.append(label)&#10;                        count += 1&#10;                    else:&#10;                        failed += 1&#10;&#10;        print(f&quot;   Loaded: {count} samples, Failed: {failed}&quot;)&#10;        return X, y&#10;&#10;    def load_train_test_split(self, train_root, test_root, augment_train=True):&#10;        &quot;&quot;&quot;&#10;        Load both training and test datasets&#10;&#10;        Args:&#10;            train_root: Root folder for training data&#10;            test_root: Root folder for test data&#10;            augment_train: Whether to augment training data&#10;&#10;        Returns:&#10;            Tuple of (X_train, y_train, X_test, y_test)&#10;        &quot;&quot;&quot;&#10;        X_train, y_train, X_test, y_test = [], [], [], []&#10;&#10;        print(&quot;\n Loading Training Data&quot; + (&quot; (augmented)&quot; if augment_train else &quot;&quot;))&#10;        for label_name, label_idx in self.labels.items():&#10;            print(f&quot;  {label_name}:&quot;, end=&quot; &quot;)&#10;            X1, y1 = self.load_dataset(&#10;                os.path.join(train_root, label_name),&#10;                label_idx,&#10;                augment=augment_train&#10;            )&#10;            X_train.extend(X1)&#10;            y_train.extend(y1)&#10;&#10;        print(&quot;\n Loading Test Data&quot;)&#10;        for label_name, label_idx in self.labels.items():&#10;            print(f&quot;  {label_name}:&quot;, end=&quot; &quot;)&#10;            X2, y2 = self.load_dataset(&#10;                os.path.join(test_root, label_name),&#10;                label_idx,&#10;                augment=False&#10;            )&#10;            X_test.extend(X2)&#10;            y_test.extend(y2)&#10;&#10;        return (np.array(X_train), np.array(y_train),&#10;                np.array(X_test), np.array(y_test))&#10;&#10;&#10;# =====================================&#10;# Model Trainer Class&#10;# =====================================&#10;class RPSModelTrainer:&#10;    &quot;&quot;&quot;Class for training and evaluating Rock Paper Scissors classifier&quot;&quot;&quot;&#10;&#10;    def __init__(self, model=None, scaler=None):&#10;        &quot;&quot;&quot;&#10;        Initialize trainer&#10;&#10;        Args:&#10;            model: Sklearn classifier (default: RidgeClassifier)&#10;            scaler: Feature scaler (default: StandardScaler)&#10;        &quot;&quot;&quot;&#10;        self.model = model if model else RidgeClassifier(random_state=42)&#10;        self.scaler = scaler if scaler else StandardScaler()&#10;        self.labels = {0: &quot;rock&quot;, 1: &quot;paper&quot;, 2: &quot;scissors&quot;}&#10;        self.best_model = None&#10;&#10;    def normalize_data(self, X_train, X_test):&#10;        &quot;&quot;&quot;&#10;        Normalize features using StandardScaler&#10;&#10;        Args:&#10;            X_train: Training features&#10;            X_test: Test features&#10;&#10;        Returns:&#10;            Tuple of (X_train_scaled, X_test_scaled)&#10;        &quot;&quot;&quot;&#10;        print(&quot;\n⚙️ Normalizing features...&quot;)&#10;        X_train_scaled = self.scaler.fit_transform(X_train)&#10;        X_test_scaled = self.scaler.transform(X_test)&#10;        return X_train_scaled, X_test_scaled&#10;&#10;    def hyperparameter_tuning(self, X_train, y_train, param_grid=None):&#10;        &quot;&quot;&quot;&#10;        Perform hyperparameter tuning using GridSearchCV&#10;&#10;        Args:&#10;            X_train: Training features (normalized)&#10;            y_train: Training labels&#10;            param_grid: Grid of parameters to search (optional)&#10;&#10;        Returns:&#10;            Best estimator found&#10;        &quot;&quot;&quot;&#10;        if param_grid is None:&#10;            param_grid = {&#10;                'alpha': [0.1, 1.0, 10.0, 100.0],&#10;                'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']&#10;            }&#10;&#10;        print(&quot;\n Performing hyperparameter tuning (this may take a while)...&quot;)&#10;&#10;        grid_search = GridSearchCV(&#10;            self.model,&#10;            param_grid,&#10;            cv=5,&#10;            scoring='accuracy',&#10;            n_jobs=-1,&#10;            verbose=1&#10;        )&#10;&#10;        grid_search.fit(X_train, y_train)&#10;&#10;        print(f&quot;\n✨ Best parameters found: {grid_search.best_params_}&quot;)&#10;        print(f&quot;✨ Best cross-validation score: {grid_search.best_score_:.4f}&quot;)&#10;&#10;        self.best_model = grid_search.best_estimator_&#10;        return self.best_model&#10;&#10;    def cross_validate(self, X_train, y_train, cv=5):&#10;        &quot;&quot;&quot;&#10;        Perform cross-validation&#10;&#10;        Args:&#10;            X_train: Training features (normalized)&#10;            y_train: Training labels&#10;            cv: Number of folds&#10;&#10;        Returns:&#10;            Cross-validation scores&#10;        &quot;&quot;&quot;&#10;        print(&quot;\n Cross-validation scores:&quot;)&#10;        model = self.best_model if self.best_model else self.model&#10;        cv_scores = cross_val_score(model, X_train, y_train, cv=cv)&#10;        print(f&quot;   CV Scores: {cv_scores}&quot;)&#10;        print(f&quot;   Mean CV Score: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})&quot;)&#10;        return cv_scores&#10;&#10;    def evaluate(self, X_test, y_test):&#10;        &quot;&quot;&quot;&#10;        Evaluate model on test set&#10;&#10;        Args:&#10;            X_test: Test features (normalized)&#10;            y_test: Test labels&#10;&#10;        Returns:&#10;            Tuple of (accuracy, predictions)&#10;        &quot;&quot;&quot;&#10;        print(&quot;\n Evaluating on test set...&quot;)&#10;        model = self.best_model if self.best_model else self.model&#10;        y_pred = model.predict(X_test)&#10;        acc = accuracy_score(y_test, y_pred)&#10;&#10;        print(f&quot;\n Test accuracy: {acc:.4f}\n&quot;)&#10;        print(&quot;Classification report:&quot;)&#10;        print(classification_report(y_test, y_pred, target_names=self.labels.values()))&#10;&#10;        return acc, y_pred&#10;&#10;    def plot_confusion_matrix(self, y_test, y_pred, save_path='model/confusion_matrix.png'):&#10;        &quot;&quot;&quot;&#10;        Plot and save confusion matrix&#10;&#10;        Args:&#10;            y_test: True labels&#10;            y_pred: Predicted labels&#10;            save_path: Path to save the plot&#10;        &quot;&quot;&quot;&#10;        cm = confusion_matrix(y_test, y_pred)&#10;        plt.figure(figsize=(8, 6))&#10;        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',&#10;                    xticklabels=self.labels.values(),&#10;                    yticklabels=self.labels.values())&#10;        plt.title('Confusion Matrix')&#10;        plt.ylabel('True Label')&#10;        plt.xlabel('Predicted Label')&#10;&#10;        os.makedirs(os.path.dirname(save_path), exist_ok=True)&#10;        plt.savefig(save_path, dpi=150, bbox_inches='tight')&#10;        print(f&quot; Confusion matrix saved to {save_path}&quot;)&#10;        plt.close()&#10;&#10;    def save_model(self, model_path='model/rps_ridge_model.joblib',&#10;                   scaler_path='model/rps_scaler.joblib'):&#10;        &quot;&quot;&quot;&#10;        Save trained model and scaler&#10;&#10;        Args:&#10;            model_path: Path to save model&#10;            scaler_path: Path to save scaler&#10;        &quot;&quot;&quot;&#10;        model = self.best_model if self.best_model else self.model&#10;&#10;        os.makedirs(os.path.dirname(model_path), exist_ok=True)&#10;        dump(model, model_path)&#10;        dump(self.scaler, scaler_path)&#10;&#10;        print(f&quot;\n Saved model to {model_path}&quot;)&#10;        print(f&quot; Saved scaler to {scaler_path}&quot;)&#10;&#10;&#10;# =====================================&#10;# Main Training Pipeline&#10;# =====================================&#10;class TrainingPipeline:&#10;    &quot;&quot;&quot;Complete training pipeline for RPS classifier&quot;&quot;&quot;&#10;&#10;    def __init__(self, train_root, test_root,&#10;                 static_image_mode=True, max_num_hands=1,&#10;                 min_detection_confidence=0.3, min_tracking_confidence=0.3):&#10;        &quot;&quot;&quot;&#10;        Initialize training pipeline&#10;&#10;        Args:&#10;            train_root: Root folder for training data&#10;            test_root: Root folder for test data&#10;            static_image_mode: Whether to treat input as static images&#10;            max_num_hands: Maximum number of hands to detect&#10;            min_detection_confidence: Minimum confidence for hand detection&#10;            min_tracking_confidence: Minimum confidence for hand tracking&#10;        &quot;&quot;&quot;&#10;        self.train_root = train_root&#10;        self.test_root = test_root&#10;&#10;        # Initialize components&#10;        self.feature_extractor = HandFeatureExtractor(&#10;            static_image_mode=static_image_mode,&#10;            max_num_hands=max_num_hands,&#10;            min_detection_confidence=min_detection_confidence,&#10;            min_tracking_confidence=min_tracking_confidence&#10;        )&#10;        self.image_augmentor = ImageAugmentor()&#10;        self.dataset_loader = RPSDatasetLoader(self.feature_extractor, self.image_augmentor)&#10;        self.trainer = RPSModelTrainer()&#10;&#10;    def run(self, augment_train=True, tune_hyperparameters=True,&#10;            param_grid=None, model_path='model/rps_ridge_model.joblib',&#10;            scaler_path='model/rps_scaler.joblib',&#10;            confusion_matrix_path='model/confusion_matrix.png'):&#10;        &quot;&quot;&quot;&#10;        Run complete training pipeline&#10;&#10;        Args:&#10;            augment_train: Whether to augment training data&#10;            tune_hyperparameters: Whether to perform hyperparameter tuning&#10;            param_grid: Grid of parameters for hyperparameter tuning&#10;            model_path: Path to save trained model&#10;            scaler_path: Path to save scaler&#10;            confusion_matrix_path: Path to save confusion matrix plot&#10;        &quot;&quot;&quot;&#10;        print(&quot;=&quot; * 60)&#10;        print(&quot; Rock Paper Scissors - Model Training Pipeline&quot;)&#10;        print(&quot;=&quot; * 60)&#10;&#10;        # Load data&#10;        X_train, y_train, X_test, y_test = self.dataset_loader.load_train_test_split(&#10;            self.train_root, self.test_root, augment_train=augment_train&#10;        )&#10;&#10;        print(f&quot;\n✅ Total Train samples: {len(X_train)} | Test samples: {len(X_test)}&quot;)&#10;        print(f&quot;   Feature dimension: {X_train.shape[1]}&quot;)&#10;&#10;        # Normalize data&#10;        X_train_scaled, X_test_scaled = self.trainer.normalize_data(X_train, X_test)&#10;&#10;        # Train model&#10;        if tune_hyperparameters:&#10;            self.trainer.hyperparameter_tuning(X_train_scaled, y_train, param_grid)&#10;        else:&#10;            print(&quot;\n Training model with default parameters...&quot;)&#10;            self.trainer.model.fit(X_train_scaled, y_train)&#10;&#10;        # Cross-validation&#10;        self.trainer.cross_validate(X_train_scaled, y_train)&#10;&#10;        # Evaluate&#10;        acc, y_pred = self.trainer.evaluate(X_test_scaled, y_test)&#10;&#10;        # Plot confusion matrix&#10;        self.trainer.plot_confusion_matrix(y_test, y_pred, confusion_matrix_path)&#10;&#10;        # Save model&#10;        self.trainer.save_model(model_path, scaler_path)&#10;&#10;        # Cleanup&#10;        self.feature_extractor.close()&#10;&#10;        print(&quot;\n Training complete!&quot;)&#10;        print(&quot;=&quot; * 60)&#10;&#10;        return acc&#10;&#10;&#10;# =====================================&#10;# Main entry point&#10;# =====================================&#10;def main(train_root, test_root, augment_train=True, tune_hyperparameters=True,&#10;         param_grid=None, model_path='model/rps_ridge_model.joblib',&#10;         scaler_path='model/rps_scaler.joblib',&#10;         confusion_matrix_path='model/confusion_matrix.png',&#10;         static_image_mode=True, max_num_hands=1,&#10;         min_detection_confidence=0.3, min_tracking_confidence=0.3):&#10;    &quot;&quot;&quot;&#10;    Main function to run training pipeline&#10;&#10;    Args:&#10;        train_root: Root folder for training data&#10;        test_root: Root folder for test data&#10;        augment_train: Whether to augment training data&#10;        tune_hyperparameters: Whether to perform hyperparameter tuning&#10;        param_grid: Grid of parameters for hyperparameter tuning&#10;        model_path: Path to save trained model&#10;        scaler_path: Path to save scaler&#10;        confusion_matrix_path: Path to save confusion matrix plot&#10;        static_image_mode: Whether to treat input as static images&#10;        max_num_hands: Maximum number of hands to detect&#10;        min_detection_confidence: Minimum confidence for hand detection&#10;        min_tracking_confidence: Minimum confidence for hand tracking&#10;    &quot;&quot;&quot;&#10;    pipeline = TrainingPipeline(&#10;        train_root, test_root,&#10;        static_image_mode, max_num_hands,&#10;        min_detection_confidence, min_tracking_confidence&#10;    )&#10;    pipeline.run(&#10;        augment_train=augment_train,&#10;        tune_hyperparameters=tune_hyperparameters,&#10;        param_grid=param_grid,&#10;        model_path=model_path,&#10;        scaler_path=scaler_path,&#10;        confusion_matrix_path=confusion_matrix_path&#10;    )&#10;&#10;&#10;if __name__ == &quot;__main__&quot;:&#10;    # =====================================&#10;    # Configuration parameters&#10;    # =====================================&#10;&#10;    # Dataset paths&#10;    TRAIN_ROOT = &quot;dataset/rps&quot;&#10;    TEST_ROOT = &quot;dataset/rps-test-set&quot;&#10;&#10;    # Model save paths&#10;    MODEL_PATH = &quot;model/rps_ridge_model.joblib&quot;&#10;    SCALER_PATH = &quot;model/rps_scaler.joblib&quot;&#10;    CONFUSION_MATRIX_PATH = &quot;model/confusion_matrix.png&quot;&#10;&#10;    # Training options&#10;    AUGMENT_TRAIN = True&#10;    TUNE_HYPERPARAMETERS = True&#10;&#10;    # Hyperparameter tuning grid&#10;    PARAM_GRID = {&#10;        'alpha': [0.1, 1.0, 10.0, 100.0],&#10;        'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']&#10;    }&#10;&#10;    # MediaPipe configuration&#10;    STATIC_IMAGE_MODE = True&#10;    MAX_NUM_HANDS = 1&#10;    MIN_DETECTION_CONFIDENCE = 0.3&#10;    MIN_TRACKING_CONFIDENCE = 0.3&#10;&#10;    # Run training&#10;    main(&#10;        train_root=TRAIN_ROOT,&#10;        test_root=TEST_ROOT,&#10;        augment_train=AUGMENT_TRAIN,&#10;        tune_hyperparameters=TUNE_HYPERPARAMETERS,&#10;        param_grid=PARAM_GRID,&#10;        model_path=MODEL_PATH,&#10;        scaler_path=SCALER_PATH,&#10;        confusion_matrix_path=CONFUSION_MATRIX_PATH,&#10;        static_image_mode=STATIC_IMAGE_MODE,&#10;        max_num_hands=MAX_NUM_HANDS,&#10;        min_detection_confidence=MIN_DETECTION_CONFIDENCE,&#10;        min_tracking_confidence=MIN_TRACKING_CONFIDENCE&#10;    )&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
      </map>
    </option>
  </component>
</project>